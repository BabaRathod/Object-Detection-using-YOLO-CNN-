{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sohanamitarathod/object-detection-using-yolo-cnn?scriptVersionId=141461242\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center><h1><u>YOLO (You Only Look Once)</center></h1></u>\n\nYou Only Look Once (YOLO) is an algorithm proposed by Redmond et al in a research study published as a conference paper at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), which won the OpenCV People's Choice Award.\n\nYOLO proposes the usage of an end-to-end neural network that provides predictions of bounding boxes and class probabilities all at once, as opposed to the method followed by object detection algorithms before YOLO, which repurposed classifiers to perform detection.\n\n\n\n\n\n\nUnder YOLO,you will be working on Trash detection project wherein the model will detect whether the image contains trash or not.","metadata":{"id":"6X9ksJWOYv6O"}},{"cell_type":"markdown","source":"<h2><u>Roadmap for the project</h2></u>\n\n* Installing YOLO and other dependencies\n* Getting the data and processing it\n* Data Annotation\n* Creating bounding boxes\n* Train the different YOLO models\n* Implement Tensorboard\n* making predictions\n","metadata":{"id":"BZl4RBrHwNth"}},{"cell_type":"code","source":"import os\n\nos.environ[\"KAGGLE_USERNAME\"] = \"sohanamitarathod\"\nos.environ[\"KAGGLE_KEY\"] = \"08afbff31dec35907c4939ae0b38ae46\"","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:33:48.493485Z","iopub.execute_input":"2023-08-18T07:33:48.494017Z","iopub.status.idle":"2023-08-18T07:33:48.50048Z","shell.execute_reply.started":"2023-08-18T07:33:48.493973Z","shell.execute_reply":"2023-08-18T07:33:48.499103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle datasets download -d kneroma/tacotrashdataset","metadata":{"executionInfo":{"elapsed":51200,"status":"ok","timestamp":1642803801122,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"M__1L7ByQFbO","outputId":"700eae33-44b9-496e-f341-bba98a4b36f1","execution":{"iopub.status.busy":"2023-08-18T07:43:28.719898Z","iopub.execute_input":"2023-08-18T07:43:28.720335Z","iopub.status.idle":"2023-08-18T07:43:46.568888Z","shell.execute_reply.started":"2023-08-18T07:43:28.7203Z","shell.execute_reply":"2023-08-18T07:43:46.565086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the file location and its path\n\nimport os\n\n# Get the current working directory\ncurrent_directory = os.getcwd()\n\n# Construct the full path to the downloaded dataset\ndataset_subdirectory = 'tacotrashdataset'  # Update with the actual subdirectory name\ndataset_path = os.path.join(current_directory, dataset_subdirectory)\n\n# Print the path to the dataset\nprint(dataset_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:44:59.269522Z","iopub.execute_input":"2023-08-18T07:44:59.270104Z","iopub.status.idle":"2023-08-18T07:44:59.280204Z","shell.execute_reply.started":"2023-08-18T07:44:59.270059Z","shell.execute_reply":"2023-08-18T07:44:59.278618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unzip the folder\n!unzip -o tacotrashdataset.zip","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:48:51.032811Z","iopub.execute_input":"2023-08-18T07:48:51.033944Z","iopub.status.idle":"2023-08-18T07:49:24.077258Z","shell.execute_reply.started":"2023-08-18T07:48:51.033891Z","shell.execute_reply":"2023-08-18T07:49:24.07514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking is extraction is properly successful\n\nimport os\n\ndataset_path = '/kaggle/working/tacotrashdataset'  # Update with the actual path\nimage_files_exist = any(f.endswith('.jpg') for f in os.listdir(dataset_path))\n\nif image_files_exist:\n    print(\"Image files exist in the dataset.\")\nelse:\n    print(\"Image files are missing in the dataset.\")","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:38:33.261444Z","iopub.execute_input":"2023-08-18T07:38:33.262017Z","iopub.status.idle":"2023-08-18T07:38:33.316856Z","shell.execute_reply.started":"2023-08-18T07:38:33.261968Z","shell.execute_reply":"2023-08-18T07:38:33.314522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clone the YoloV5 github for using its architecture and for detection","metadata":{"id":"oYfqPM9FZYzf"}},{"cell_type":"markdown","source":"*YOLOv5 ðŸš€ is a family of object detection architectures and models pretrained on the COCO dataset, and represents Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.*","metadata":{"id":"JVVd3JfnZv4-"}},{"cell_type":"markdown","source":"Documentation\nSee the [YOLOv5 Docs](https://docs.ultralytics.com/) for full documentation on training, testing and deployment. bold text","metadata":{"id":"v1PiylREZ2gC"}},{"cell_type":"markdown","source":"### **Step 1: Install Yolo**","metadata":{"id":"orxJeu_nZ2c3"}},{"cell_type":"markdown","source":"\n\n1.  Clone the Yolo repository.\n2.  Go to yolov5 folder.\n3.  Install all the requirements for running the architecture.\n\n\n","metadata":{"id":"4YMGZ62ZaYrF"}},{"cell_type":"code","source":"# Install Required Packages:\n!pip install torch torchvision opencv-python\n# Clone the YOLO Repository:\n!git clone https://github.com/ultralytics/yolov5.git\n# Navigate to the YOLO Directory:\n%cd yolov5\n# Install Required Packages for YOLOv5:\n!pip install -U -r requirements.txt\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:12:59.844916Z","iopub.execute_input":"2023-08-18T08:12:59.845976Z","iopub.status.idle":"2023-08-18T08:18:30.841508Z","shell.execute_reply.started":"2023-08-18T08:12:59.845929Z","shell.execute_reply":"2023-08-18T08:18:30.839882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Install pycocotools \n(pycocotools is a Python API that assists in loading, parsing and visualizing the annotations in COCO.)**\n\nRefer: https://pypi.org/project/pycocotools/","metadata":{"id":"y9aqGXE8axvh"}},{"cell_type":"code","source":"#install pycocotools\n!pip install pycocotools\n","metadata":{"executionInfo":{"elapsed":2424,"status":"ok","timestamp":1642803806770,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"YhEDLOjWQi-5","outputId":"98155e42-54b7-4370-92f5-167c313b2fd7","execution":{"iopub.status.busy":"2023-08-18T08:20:03.914357Z","iopub.execute_input":"2023-08-18T08:20:03.914936Z","iopub.status.idle":"2023-08-18T08:20:06.608065Z","shell.execute_reply.started":"2023-08-18T08:20:03.914894Z","shell.execute_reply":"2023-08-18T08:20:06.605851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Import the required libraries**\n\n* torch: for tensor computation\n* Image: for working with images\n* os: to handle and work with the operating system\n* random: used to generate random numbers,https://docs.python.org/3/library/random.html\n* shutil: offers high-level operation on a file like a copy, create, and remote operation on the file,https://docs.python.org/3/library/shutil.html\n* train_test_split: to split the data into training and testing data\n* ElementTree: represents the whole XML document as a tree,https://docs.python.org/3/library/xml.etree.elementtree.html\n* minidom: implementation of the Document Object Model interface, https://docs.python.org/3/library/xml.dom.minidom.html\n* tqdm: used for creating Progress Meters or Progress Bars,https://tqdm.github.io/\n* ImageDraw: provides simple 2D graphics for Image objects,https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html\n* numpy: for array and matrix operations\n* matplotlib: for visualizations\n* yolov5.utils: https://github.com/ultralytics/yolov5","metadata":{"id":"Cb6__P-Kc7CX"}},{"cell_type":"code","source":"!pip install --upgrade numpy","metadata":{"execution":{"iopub.status.busy":"2023-08-18T09:00:27.621907Z","iopub.execute_input":"2023-08-18T09:00:27.623292Z","iopub.status.idle":"2023-08-18T09:00:44.776855Z","shell.execute_reply.started":"2023-08-18T09:00:27.623225Z","shell.execute_reply":"2023-08-18T09:00:44.775185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport xml.etree.ElementTree as ET\nimport xml.dom.minidom\nfrom tqdm import tqdm\nfrom PIL import ImageDraw\nimport numpy as np\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:48:59.685363Z","iopub.execute_input":"2023-08-18T08:48:59.686031Z","iopub.status.idle":"2023-08-18T08:49:07.307341Z","shell.execute_reply.started":"2023-08-18T08:48:59.685982Z","shell.execute_reply":"2023-08-18T08:49:07.304309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#install above mentioned libraries\nimport torch\nfrom PIL import Image\nimport os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport xml.etree.ElementTree as ET\nimport xml.dom.minidom\nfrom tqdm import tqdm\nfrom PIL import ImageDraw\nimport numpy as np\nimport matplotlib.pyplot as plt\n#set random seed to 108\n","metadata":{"id":"yJm-LGh6QjCK","execution":{"iopub.status.busy":"2023-08-18T09:01:04.344088Z","iopub.execute_input":"2023-08-18T09:01:04.344632Z","iopub.status.idle":"2023-08-18T09:01:06.078406Z","shell.execute_reply.started":"2023-08-18T09:01:04.34459Z","shell.execute_reply":"2023-08-18T09:01:06.07676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cat command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files.\nUse cat to view dataset in json format and for custom datas.","metadata":{"id":"OCRBuWxudBJf"}},{"cell_type":"code","source":"#view annotations.json using cat command\n!cat /kaggle/working/data/annotations.json","metadata":{"executionInfo":{"elapsed":6645,"status":"ok","timestamp":1642803819733,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"rsV87GMydP4G","outputId":"dae7938c-c1e0-4899-f0b5-9b1c26b3c4cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <u>Train On Custom Data</u>\n\n1. Create dataset.yaml.\n2. [COCO128](https://www.kaggle.com/ultralytics/coco128) is a small tutorial dataset composed of the first 128 images in COCO train2017. \n3. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.\n4. yaml, shown below, is the dataset configuration file that defines \n\n>* an optional download command/URL for auto-downloading, \n>* a path to a directory of training images (or path to a *.txt file with a list of training images), \n>* the same for our validation images, \n>* the number of classes, \n>* a list of class names:\n\n\n\n\n\n\n\n","metadata":{"id":"MfUg709keLd9"}},{"cell_type":"code","source":"#import COCO\n!pip install pycocotools\nfrom pycocotools.coco import COCO","metadata":{"executionInfo":{"elapsed":1512,"status":"ok","timestamp":1642803819734,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"HP07BugzQjFc","outputId":"aee58633-627e-41be-f893-27d810d71ddb","execution":{"iopub.status.busy":"2023-08-18T09:07:40.323002Z","iopub.execute_input":"2023-08-18T09:07:40.323486Z","iopub.status.idle":"2023-08-18T09:07:57.509189Z","shell.execute_reply.started":"2023-08-18T09:07:40.323447Z","shell.execute_reply":"2023-08-18T09:07:57.507389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the annotations.json using COCO\nannotations_path = '/kaggle/working/data/annotations.json'  # Update with the correct path\ncoco = COCO(annotations_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T09:10:45.857531Z","iopub.execute_input":"2023-08-18T09:10:45.857947Z","iopub.status.idle":"2023-08-18T09:10:45.92922Z","shell.execute_reply.started":"2023-08-18T09:10:45.857913Z","shell.execute_reply":"2023-08-18T09:10:45.927732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check all the categories availiable in given trash detection dataset with id and categories**\n\nUse the 'cats' method to do so.","metadata":{"id":"4wuZcyeoe8sU"}},{"cell_type":"code","source":"#view categories\n","metadata":{"executionInfo":{"elapsed":73,"status":"ok","timestamp":1642803819735,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"-IKrp_efQjIF","outputId":"72bc6f34-ee31-46d3-b482-684ca6dedffc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extract names from given dataset and print it**\n\nNow extract the categories of trash from the given data by iterating through it and extracting the name attributes. ","metadata":{"id":"LtLr1j6ggdAc"}},{"cell_type":"code","source":"#iterate using enumerate\n\n  #print the number of category and name of trash\n  ","metadata":{"executionInfo":{"elapsed":67,"status":"ok","timestamp":1642803819736,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"3mASPIn2QjK9","outputId":"f89354bf-5b3b-45d3-b900-413a01a313b4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see there are 59 categories of trash. \nCreate a dictionary named as label_transfer.","metadata":{"id":"nyLtw3r9gkCu"}},{"cell_type":"code","source":"#label_transfer = {5: 0, 12: 1}\n","metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1642803819737,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"Rpmn3Z16QjN3","outputId":"9f009d0e-d095-4af6-9555-079481ae8763"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"coco returns the dictionary of the dataset. Here we can get id's of images with coco.getImgIds() function, after getting image id's we have to load those images for loading the images we can use coco.loadImgs()","metadata":{"id":"KmHN4e7DhfFN"}},{"cell_type":"code","source":"#get the image IDs\n","metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1642803819738,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"dAwhZUwAQjQu","outputId":"7304ae50-515b-402b-843b-306db724d15e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Category IDs gives number of unique ids in given dataset format**","metadata":{"id":"qbk0v8aPh9tN"}},{"cell_type":"code","source":"#get category IDs\n","metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1642803819739,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"jmXRBtK-QjTl","outputId":"9ba179e8-1974-4c8b-df58-e7308d823062"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading categories using catIds for finding category names","metadata":{"id":"rGbxcbJMiZH9"}},{"cell_type":"code","source":"#use loadCats methods and pass the extracted category IDs\n","metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1642803819739,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"eqHoqIh1QjWf","outputId":"1994696d-9177-4abe-9ad8-9de77fadc462"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sort category based on id using [lambda](https://www.w3schools.com/python/python_lambda.asp) functions","metadata":{"id":"77wHQkcei0eM"}},{"cell_type":"code","source":"#sort the categories\n","metadata":{"id":"trsi2-XFQjZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create an empty dictonary for labels, classes, label inverse that we need to find in our dataset","metadata":{"id":"IOs-XbrYjDPl"}},{"cell_type":"code","source":"#create classes dictionary\n\n#create coco_labels dictionary\n\n#create coco_labels_inverse dictionary\n","metadata":{"id":"xN1jHgPNgH-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next run a for loop iterating through the sorted categories. Store the category IDs in coco_labels dictionary(keys as len(classes) and values as category ID) and IDs in inverse order in the coco_labels_inverse dictionary(keys as category ID and values as len(classes)). Store the length of classes in classes dictionary(keys as category name and values as len(classes)).","metadata":{"id":"hVLFctHIwNuP"}},{"cell_type":"code","source":"#iterate through categories\n\n    #store category IDs in coco_labels(keys-->len(classes))\n    \n    #store len(classes) in coco_labels_inverse(keys-->c['id'])\n    \n    #store len(classes) in classes(keys-->c['name'])\n    ","metadata":{"id":"S8deSIcSgJ54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"View the classes dictionary","metadata":{"id":"YK9SAP4RwNuR"}},{"cell_type":"code","source":"#print classes\n","metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1642803819742,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"rXHMNs5hgLv7","outputId":"4977724f-f6f1-431f-d66a-d1d61f9811f8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#class_num = {}\n","metadata":{"id":"u9RimRyygObT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new directory for storing images and labels using the mkdir command(make directory)","metadata":{"id":"huh80qDfj0ab"}},{"cell_type":"code","source":"#!mkdir -p tmp/labels tmp/images\n\n#save_base_path  = 'tmp/labels/'\n\n#save_image_path = 'tmp/images/'\n","metadata":{"id":"AkhVCEDygRMk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert .json file to .txt file\nAnnotate the images using the .txt file obtained and make a binding box aroung the object.","metadata":{"id":"11Nza7HWj_1l"}},{"cell_type":"markdown","source":"### **Bounding Boxes**\nIn object detection, we usually use a bounding box to describe the spatial location of an object. The bounding box is rectangular, which is determined by the  x  and  y  coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. Another commonly used bounding box representation is the  (x,y) -axis coordinates of the bounding box center, and the width and height of the box.<br><br>\n- The code starts by enumerating all the images in a data source.\n- The first one is img_id=0, which is the image that was loaded from the data source.\n- Then it creates a variable called \"img_info\" and stores the information about this image in it.\n- Next, it saves this image's file name to a new file with \"_\" as its extension so that when you open up this text file later on, you can see what changes were made to your original .json file.\n\n- Next, it creates another variable called \"save_name\".\n- This will be used for saving any other files that are created during this process of changing an existing json file into a txt-file.\n- It then uses split() to separate out the filename from its extension and store them separately in variables called height and width respectively.\n- Finally, save_base_path + filename + '.txt' is saved to disk at whatever location you want (I chose my desktop).\n\n- The next line checks if there already exists an empty text document with these two variables as its name; if not then create one now using open().- The code will change the .json file to a .txt file.- The code starts by loading the data from the file.\n- The code then goes through each of the annotations and labels them with their corresponding class number.\n- If there is an annotation that does not have a label, it will be labeled as 0.\n- The next part of the code loops through all of the boxes in each annotation and writes out its coordinates to a list called lines.- The code attempts to create a list of the annotation ids that are present in the image.\n\n- The code then iterates through each annotation id and loads it from the data source.\n\n- Next, it creates a list of strings which will be used as labels for each box in the plot.\n\n- Lastly, if there are any annotations that have not been transferred yet, they will be added to class_num with a label of 0 and 1 respectively.\nâ€“\n","metadata":{"id":"QLdUZsAskmhM"}},{"cell_type":"code","source":"#iterate through image IDs and show tqdm progress bar\n#for index, img_id in tqdm.tqdm(enumerate(img_ids), desc='change .json file to .txt file')\n\n    #get image info,data_source.loadImgs(img_id)[0]\n    \n    #replace '/' with '_' in file name of images\n    \n    #split the file name based on '.'\n    \n   #get height of image\n    \n    #get width of image\n    \n    #save new path of image \n    #save_path = save_base_path + file_name + '.txt'\n    \n    #set is_exist to False\n    \n    #open the saved path in write mode\n    \n        #get the annotation ID\n        \n        #create boxes,np.zeros((0, 5))\n        \n        #check if length of annotation IDs is 0\n        \n            #insert ''\n            #fp.write('')\n            \n            #continue\n            \n        #load annotations\n        #annotations = data_source.loadAnns(annotation_id)\n        \n        #set a blank string 'lines'\n         \n        #iterate through annotations\n        \n            #extract label from coco_labels_inverse\n            #label = coco_labels_inverse[annotation['category_id']]\n           \n            #if label in label_transfer.keys():\n            \n                #set is_exist to True\n                \n                #get bbox of annotation\n                #box = annotation['bbox']\n                \n                #check if box[2] < 1 or box[3] < 1:\n                \n                   #continue\n                    \n                #create boxes\n                # box[0] = round((box[0] + box[2] / 2) / width, 6)\n                #box[1] = round((box[1] + box[3] / 2) / height, 6)\n                #box[2] = round(box[2] / width, 6)\n                #box[3] = round(box[3] / height, 6)\n                \n\n\n\n                #label = label_transfer[label]\n                 \n                #check if label is present in classes\n                #if label not in class_num.keys():\n                \n                    #class_num[label] = 0\n                    \n                #increment class_num[label]\n                \n                #add the string of label to lines\n                \n                #iterate through box\n                #for i in box:\n                \n                    #add the string of each box in lines\n                    #lines += ' ' + str(i)\n                    \n                #add a blank line in lines\n               \n        #add these lines to the file\n        #fp.writelines(lines)\n        \n    #check if is_exist is True\n    \n        #copy the dataset paths and image paths\n        #shutil.copy('/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/unzipped_taco_dataset/data/{}'.format(img_info['file_name']), os.path.join(save_image_path, save_name))\n        \n    #else\n      \n        #remove save_path\n        ","metadata":{"executionInfo":{"elapsed":240659,"status":"ok","timestamp":1642804060375,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"3Y19s7T4gTBu","outputId":"10a76758-3d26-4198-8ae8-f6c6faf23f71"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Split the dataset into train test valid using split-folder library**","metadata":{"id":"xYVNrQQBk7xE"}},{"cell_type":"code","source":"#install split-folders\n","metadata":{"executionInfo":{"elapsed":2805,"status":"ok","timestamp":1642804063159,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"vGlrZ_tegjqN","outputId":"fa36efd4-12bd-4df4-c434-4618516d32b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Split data into:-\n80% train <br>\n10% test<br>\n10% validate\n\nhttps://pypi.org/project/split-folders/","metadata":{"id":"85o-6APylF9T"}},{"cell_type":"code","source":"#import splitfolders\n\n#split the folders\n#splitfolders.ratio('tmp', output=\"taco\", seed=1337, ratio=(.8, 0.1,0.1)) \n","metadata":{"executionInfo":{"elapsed":160379,"status":"ok","timestamp":1642804223527,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"VV3Mgmmvglt1","outputId":"f7eb1aae-b81f-4718-d56f-349f91a6f6fd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train the model created**","metadata":{"id":"iDnvtwKJlZXC"}},{"cell_type":"markdown","source":"* The commands below reproduce YOLOv5 COCO results. Models and datasets download automatically from the latest YOLOv5 release. \n* Training times for YOLOv5n/s/m/l/x are 1/2/4/6/8 days on a V100 GPU (Multi-GPU times faster).\n* Use the largest --batch-size possible, or pass --batch-size -1 for YOLOv5 AutoBatch. \n* Batch sizes shown for V100-16GB.\n\npython train.py --data coco.yaml --cfg yolov5n.yaml --weights '' --batch-size 128","metadata":{"id":"MVuhc6a5lqO7"}},{"cell_type":"markdown","source":"\n|Model |size<br><sup>(pixels) |mAP<sup>val<br>0.5:0.95 |mAP<sup>val<br>0.5 |Speed<br><sup>CPU b1<br>(ms) |Speed<br><sup>V100 b1<br>(ms) |Speed<br><sup>V100 b32<br>(ms) |params<br><sup>(M) |FLOPs<br><sup>@640 (B)\n|---                    |---  |---    |---    |---    |---    |---    |---    |---\n|[YOLOv5n][assets]      |640  |28.4   |46.0   |**45** |**6.3**|**0.6**|**1.9**|**4.5**\n|[YOLOv5s][assets]      |640  |37.2   |56.0   |98     |6.4    |0.9    |7.2    |16.5\n|[YOLOv5m][assets]      |640  |45.2   |63.9   |224    |8.2    |1.7    |21.2   |49.0\n|[YOLOv5l][assets]      |640  |48.8   |67.2   |430    |10.1   |2.7    |46.5   |109.1\n|[YOLOv5x][assets]      |640  |50.7   |68.9   |766    |12.1   |4.8    |86.7   |205.7\n|                       |     |       |       |       |       |       |       |\n|[YOLOv5n6][assets]     |1280 |34.0   |50.7   |153    |8.1    |2.1    |3.2    |4.6\n|[YOLOv5s6][assets]     |1280 |44.5   |63.0   |385    |8.2    |3.6    |12.6   |16.8\n|[YOLOv5m6][assets]     |1280 |51.0   |69.0   |887    |11.1   |6.8    |35.7   |50.0\n|[YOLOv5l6][assets]     |1280 |53.6   |71.6   |1784   |15.8   |10.5   |76.7   |111.4\n|[YOLOv5x6][assets]<br>+ [TTA][TTA]|1280<br>1536 |54.7<br>**55.4** |**72.4**<br>72.3 |3136<br>- |26.2<br>- |19.4<br>- |140.7<br>- |209.8<br>-","metadata":{"id":"nNZaDpJXm77e"}},{"cell_type":"markdown","source":"# Different Yolo models comparisons\n<a href=\"https://ibb.co/26nzD5T\"><img src=\"https://i.ibb.co/8BKyWXH/model-comparison.png\" alt=\"model-comparison\" border=\"0\"></a>","metadata":{"id":"SPvRsAxPmWVM"}},{"cell_type":"markdown","source":"Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release.\n\nExample for training YOLOv5s on COCO128 for 3 epochs<br>\n$ python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt\n\nAll training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2, runs/train/exp3 etc. For more details see the Training section of our tutorial notebook. ","metadata":{"id":"zMjNdUCxnLWT"}},{"cell_type":"code","source":"#!python /content/drive/MyDrive/ColabNote/trash_detect/yolov5/train.py --img 320 --batch 1 --epochs 10 --data /content/drive/MyDrive/ColabNote/trash_detect/yolov5/taco8.yaml --cfg /content/drive/MyDrive/ColabNote/trash_detect/yolov5/models/yolov5s.yaml --weights yolov5s.pt --cache\n","metadata":{"executionInfo":{"elapsed":292835,"status":"ok","timestamp":1642804516338,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"8aWmegZn1VrX","outputId":"edea08c4-4c63-4a58-80be-b903a67564d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [Tensorboard](https://www.tensorflow.org/tensorboard/get_started)\n##Local Logging<br>\nAll results are logged by default to runs/train, with a new experiment directory created for each new training as runs/train/exp2, runs/train/exp3, etc. View train and val jpgs to see mosaics, labels, predictions and augmentation effects.","metadata":{"id":"BRh7myDHnlNy"}},{"cell_type":"markdown","source":"TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n* Tracking and visualizing metrics such as loss and accuracy\n* Visualizing the model graph (ops and layers)\n* Viewing histograms of weights, biases, or other tensors as they change over time\n* Projecting embeddings to a lower dimensional space\n* Displaying images, text, and audio data\n* Profiling TensorFlow programs\n\nRefer:https://www.tensorflow.org/tensorboard","metadata":{"id":"kdh9P4gennyD"}},{"cell_type":"code","source":"# Tensorboard  (optional)\n#%load_ext tensorboard\n#%tensorboard --logdir /content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/train\n","metadata":{"executionInfo":{"elapsed":5056,"status":"ok","timestamp":1642804521380,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"CG7Dq9mY6McB","outputId":"916f6c55-011e-4cd6-a6ab-432d70b359c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#copy weights\n#!cp /content/drive/MyDrive/ColabNote/trash_detect/yolov5/runs/train/exp/weights/best.pt\n","metadata":{"id":"bYw4Jf_YBKAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#view files\n#!ls /content/drive/MyDrive/ColabNote/trash_detect/taco/val/images\n","metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1642804522049,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"},"user_tz":480},"id":"k_bKt6A-1Vhf","outputId":"b9efdb36-7115-4637-c035-d7bcca3a1e35"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use ipython library to display the images","metadata":{"id":"oNw2-U44oPJd"}},{"cell_type":"code","source":"#import Image\n","metadata":{"id":"4GGV-IBY_B_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check sample images in our dataset","metadata":{"id":"cuNSpNLvANA8"}},{"cell_type":"code","source":"#display any image\n","metadata":{"id":"dX4UM9AI_CFa","executionInfo":{"status":"ok","timestamp":1642804526376,"user_tz":480,"elapsed":4335,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"1ad124ea-3278-4c0f-9448-e6987ed863e0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Lets make our predictions**","metadata":{"id":"cLdB6mJ2AYLe"}},{"cell_type":"code","source":"#!python \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/detect.py\" --weights \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/train/exp/weights/best.pt\" --img 416 --conf 0.1 --source \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/taco/val/images\"\n","metadata":{"id":"ySZbzW5L_CI2","executionInfo":{"status":"ok","timestamp":1642804544503,"user_tz":480,"elapsed":18157,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"8833e417-b800-4c1f-e426-9ddb69a52a34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/detect.py\" --weights \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/train/exp/weights/best.pt\" --img 416 --conf 0.1 --source \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/taco/test/yolov5/detect.py\" --weights \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/train/exp/weights/best.pt\" --img 416 --conf 0.1 --source \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/taco/test/images\"\n","metadata":{"id":"QQzX66X__CLH","executionInfo":{"status":"ok","timestamp":1642804708836,"user_tz":480,"elapsed":18523,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"343e760b-e4c3-4d72-a0ba-05070c85e937"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/detect.py\" --source \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/taco/train/images/batch_10_000015.jpg\" --weights \"/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/train/exp/weights/best.pt\"\n","metadata":{"id":"8SzADOpg_CNf","executionInfo":{"status":"ok","timestamp":1642804554186,"user_tz":480,"elapsed":7162,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"f7a2fbce-2979-471f-f3b4-3c6a49437325"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, display our ground truth data\n","metadata":{"id":"UnicqBa-1VGi","executionInfo":{"status":"ok","timestamp":1642804560546,"user_tz":480,"elapsed":6380,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"666ab260-bc24-431c-948c-0d940f568231"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, display our ground truth data\n","metadata":{"id":"4IhXVIsuhTid","executionInfo":{"status":"ok","timestamp":1642804560548,"user_tz":480,"elapsed":104,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}},"outputId":"9776cc16-d89e-4b80-e3db-d369a02529ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, display our ground truth data\n","metadata":{"id":"da6dFE7H-43d","outputId":"9f339d07-1ced-4031-85e7-de3149e95e8c","executionInfo":{"status":"ok","timestamp":1642804560550,"user_tz":480,"elapsed":102,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, display our ground truth data\n","metadata":{"id":"qgpRj3nI_s3c","outputId":"fc5f4dd1-2462-4f15-9fd2-163e26fd6802","executionInfo":{"status":"ok","timestamp":1642804590271,"user_tz":480,"elapsed":29774,"user":{"displayName":"Simran Surve","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gik7iyGSKoZ5so9XipsZPApGM6kN-LWjBQUkZ8W9w=s64","userId":"08759634073497249150"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence we can observe that trash is detected and bounding boxes are drawn around it. Lets display all the images. Import glob,the glob module is used to retrieve files/pathnames matching a specified pattern.https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/.","metadata":{"id":"WFFYggcgwNu-"}},{"cell_type":"code","source":"#import glob,Image and display\n\n#iterate through images\n#for imageName in glob.glob('/content/drive/MyDrive/CloudyML_Time Series Analysis/Trash Detection/yolov5/runs/detect/exp5/*.jpg'): \n                                                                                                                    #assuming JPG\n    #display the image\n    ","metadata":{"id":"UMSJr3X0_8o-","outputId":"ac63e6cf-c498-4af0-bd2a-f245186398dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Congratulations!!! You've done it. \n\nIn this assignment you implemented:\n\n* YOLO\n* Trash detection project\n\n\nKeep practising!!<br><br> \n\n## Do fill the feedback form given below:\n[Feedback form](https://forms.zohopublic.in/cloudyml/form/CloudyMLDeepLearningFeedbackForm/formperma/VCFbldnXAnbcgAIl0lWv2blgHdSldheO4RfktMdgK7)\n<br><br> \n![Goodbye-Keep-Up-The-Good-Work-Good-Bye-Meme.jpg](https://m.media-amazon.com/images/I/31Tvvzb1dDL.jpg)","metadata":{"id":"WBcCMLSxAp4E"}},{"cell_type":"code","source":"","metadata":{"id":"GsFcqQABwNvB"},"execution_count":null,"outputs":[]}]}